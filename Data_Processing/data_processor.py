# Sui vs Aptos æ•¸æ“šè™•ç†å’Œæ¸…ç†å·¥å…·

import pandas as pd
import numpy as np
import json
from datetime import datetime, timedelta
import os
import logging
import warnings
import requests
import time
warnings.filterwarnings('ignore')

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SuiAptosDataProcessor:
    """Sui vs Aptos æ•¸æ“šè™•ç†å™¨"""
    
    def __init__(self, raw_data_dir="../Data_Collection/raw_data", output_dir="processed_data"):
        self.raw_data_dir = raw_data_dir
        self.output_dir = output_dir
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        for subdir in ['daily', 'weekly', 'monthly', 'analysis_ready']:
            os.makedirs(f"{output_dir}/{subdir}", exist_ok=True)
        
        logger.info(f"æ•¸æ“šè™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"åŸå§‹æ•¸æ“šç›®éŒ„: {raw_data_dir}")
        logger.info(f"è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    def load_raw_data(self):
        """è¼‰å…¥åŸå§‹æ•¸æ“š - å¾DefiLlama APIç²å–æœ€æ–°çš„éˆç´šåˆ¥æ•¸æ“š"""
        logger.info("é–‹å§‹å¾DefiLlama APIè¼‰å…¥æœ€æ–°æ•¸æ“š...")
        
        data = {}
        
        try:
            # 1. ç²å–éˆç´šåˆ¥çš„TVLæ•¸æ“š
            logger.info("æ­£åœ¨ç²å–éˆç´šåˆ¥TVLæ•¸æ“š...")
            chains_response = requests.get("https://api.llama.fi/v2/chains", timeout=30)
            chains_response.raise_for_status()
            chains_data = chains_response.json()
            
            # æ‰¾å‡ºSuiå’ŒAptosçš„ç•¶å‰TVL
            sui_chain_tvl = 0
            aptos_chain_tvl = 0
            
            for chain in chains_data:
                if chain.get('name') == 'Sui':
                    sui_chain_tvl = chain.get('tvl', 0)
                elif chain.get('name') == 'Aptos':
                    aptos_chain_tvl = chain.get('tvl', 0)
            
            logger.info(f"éˆç´šåˆ¥TVL - Sui: ${sui_chain_tvl/1e9:.2f}B, Aptos: ${aptos_chain_tvl/1e9:.2f}B")
            
            # 2. ç²å–å…·é«”å”è­°æ•¸æ“š (åƒ…ç”¨æ–¼åˆ†é¡åˆ†æ)
            logger.info("æ­£åœ¨ç²å–å”è­°è©³ç´°æ•¸æ“š...")
            protocols_response = requests.get("https://api.llama.fi/protocols", timeout=30)
            protocols_response.raise_for_status()
            all_protocols = protocols_response.json()
            
            # 3. éæ¿¾ä¸¦èª¿æ•´å”è­°TVL (æŒ‰éˆåˆ†é…)
            sui_protocols = []
            aptos_protocols = []
            
            sui_total_from_protocols = 0
            aptos_total_from_protocols = 0
            
            for protocol in all_protocols:
                chains = protocol.get('chains', [])
                protocol_tvl = float(protocol.get('tvl', 0)) if protocol.get('tvl') is not None else 0
                
                if 'Sui' in chains and protocol_tvl > 0:
                    # å¦‚æœå”è­°åœ¨å¤šå€‹éˆä¸Šï¼Œéœ€è¦æŒ‰æ¯”ä¾‹åˆ†é…TVL
                    chain_allocation = 1.0 / len(chains) if len(chains) > 1 else 1.0
                    allocated_tvl = protocol_tvl * chain_allocation
                    
                    protocol_data = {
                        'name': protocol.get('name', ''),
                        'slug': protocol.get('slug', ''),
                        'category': protocol.get('category', 'Unknown'),
                        'tvl_usd': allocated_tvl,
                        'original_tvl': protocol_tvl,
                        'chain_count': len(chains),
                        'allocation_ratio': chain_allocation,
                        'change_1d': float(protocol.get('change_1d', 0)) if protocol.get('change_1d') is not None else 0.0,
                        'change_7d': float(protocol.get('change_7d', 0)) if protocol.get('change_7d') is not None else 0.0,
                        'change_1m': float(protocol.get('change_1m', 0)) if protocol.get('change_1m') is not None else 0.0,
                        'chains_count': len(chains),
                        'chain': 'Sui',
                        'collected_date': datetime.now().strftime('%Y-%m-%d')
                    }
                    sui_protocols.append(protocol_data)
                    sui_total_from_protocols += allocated_tvl
                
                if 'Aptos' in chains and protocol_tvl > 0:
                    # å¦‚æœå”è­°åœ¨å¤šå€‹éˆä¸Šï¼Œéœ€è¦æŒ‰æ¯”ä¾‹åˆ†é…TVL
                    chain_allocation = 1.0 / len(chains) if len(chains) > 1 else 1.0
                    allocated_tvl = protocol_tvl * chain_allocation
                    
                    protocol_data = {
                        'name': protocol.get('name', ''),
                        'slug': protocol.get('slug', ''),
                        'category': protocol.get('category', 'Unknown'),
                        'tvl_usd': allocated_tvl,
                        'original_tvl': protocol_tvl,
                        'chain_count': len(chains),
                        'allocation_ratio': chain_allocation,
                        'change_1d': float(protocol.get('change_1d', 0)) if protocol.get('change_1d') is not None else 0.0,
                        'change_7d': float(protocol.get('change_7d', 0)) if protocol.get('change_7d') is not None else 0.0,
                        'change_1m': float(protocol.get('change_1m', 0)) if protocol.get('change_1m') is not None else 0.0,
                        'chains_count': len(chains),
                        'chain': 'Aptos',
                        'collected_date': datetime.now().strftime('%Y-%m-%d')
                    }
                    aptos_protocols.append(protocol_data)
                    aptos_total_from_protocols += allocated_tvl
            
            # 4. å¦‚æœå”è­°ç¸½å’Œèˆ‡éˆTVLä¸åŒ¹é…ï¼Œé€²è¡Œèª¿æ•´
            if sui_total_from_protocols > 0 and sui_chain_tvl > 0:
                sui_adjustment_factor = sui_chain_tvl / sui_total_from_protocols
                for protocol in sui_protocols:
                    protocol['tvl_usd'] *= sui_adjustment_factor
                    
            if aptos_total_from_protocols > 0 and aptos_chain_tvl > 0:
                aptos_adjustment_factor = aptos_chain_tvl / aptos_total_from_protocols
                for protocol in aptos_protocols:
                    protocol['tvl_usd'] *= aptos_adjustment_factor
            
            logger.info(f"èª¿æ•´å¾Œå”è­°TVLç¸½å’Œ - Sui: ${sum(p['tvl_usd'] for p in sui_protocols)/1e9:.2f}B")
            logger.info(f"èª¿æ•´å¾Œå”è­°TVLç¸½å’Œ - Aptos: ${sum(p['tvl_usd'] for p in aptos_protocols)/1e9:.2f}B")
            
            # 5. è½‰æ›ç‚ºDataFrame
            data['sui_protocols'] = pd.DataFrame(sui_protocols)
            data['aptos_protocols'] = pd.DataFrame(aptos_protocols)
            
            # 6. ç²å–æ­·å²TVLæ•¸æ“šä½œç‚ºåƒ¹æ ¼æ•¸æ“šçš„æ›¿ä»£
            logger.info("æ­£åœ¨ç²å–æ­·å²TVLæ•¸æ“š...")
            
            sui_price_data = self._get_historical_tvl_data('Sui')
            aptos_price_data = self._get_historical_tvl_data('Aptos')
            
            data['sui_price'] = pd.DataFrame(sui_price_data)
            data['aptos_price'] = pd.DataFrame(aptos_price_data)
            
            # 7. å‰µå»ºæ¯”è¼ƒæ•¸æ“š
            data['comparison'] = {
                'data_source': 'DefiLlama_Chain_API',
                'collection_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'sui_protocol_count': len(sui_protocols),
                'aptos_protocol_count': len(aptos_protocols),
                'sui_chain_tvl': sui_chain_tvl,
                'aptos_chain_tvl': aptos_chain_tvl
            }
            
            logger.info("APIæ•¸æ“šè¼‰å…¥æˆåŠŸ:")
            logger.info(f"  Suiå”è­°: {len(data['sui_protocols'])} å€‹")
            logger.info(f"  Aptoså”è­°: {len(data['aptos_protocols'])} å€‹")
            logger.info(f"  SUIæ­·å²æ•¸æ“š: {len(data['sui_price'])} å¤©")
            logger.info(f"  APTæ­·å²æ•¸æ“š: {len(data['aptos_price'])} å¤©")
            
            # 8. ä¿å­˜å‚™ä»½
            self._save_raw_data_backup(data)
            
            return data
            
        except requests.RequestException as e:
            logger.error(f"APIè«‹æ±‚å¤±æ•—: {e}")
            logger.info("å˜—è©¦è¼‰å…¥æœ¬åœ°å‚™ä»½æ•¸æ“š...")
            return self._load_backup_data()
            
        except Exception as e:
            logger.error(f"æ•¸æ“šè¼‰å…¥å¤±æ•—: {e}")
            return None
    
    def _get_historical_tvl_data(self, chain_name):
        """ç²å–éˆçš„æ­·å²TVLæ•¸æ“šä½œç‚ºåƒ¹æ ¼åˆ†æçš„åŸºç¤"""
        try:
            # ä½¿ç”¨æ­·å²éˆTVL API
            url = f"https://api.llama.fi/v2/historicalChainTvl/{chain_name}"
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            historical_data = response.json()
            price_data = []
            
            # è½‰æ›æ­·å²TVLæ•¸æ“šç‚ºåƒ¹æ ¼æ ¼å¼ (ä½¿ç”¨TVLä½œç‚º"åƒ¹æ ¼"çš„ä»£ç†)
            for data_point in historical_data:
                if isinstance(data_point, dict) and 'date' in data_point and 'tvl' in data_point:
                    price_data.append({
                        'date': data_point['date'],
                        'price_usd': float(data_point['tvl']) / 1e9,  # ä½¿ç”¨TVL(B)ä½œç‚ºä»£ç†åƒ¹æ ¼
                        'market_cap_usd': float(data_point['tvl']),
                        'volume_24h_usd': 0,
                        'chain': chain_name
                    })
                elif isinstance(data_point, list) and len(data_point) >= 2:
                    # è™•ç† [timestamp, tvl] æ ¼å¼
                    timestamp, tvl = data_point[0], data_point[1]
                    date = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')
                    price_data.append({
                        'date': date,
                        'price_usd': float(tvl) / 1e9,  # ä½¿ç”¨TVL(B)ä½œç‚ºä»£ç†åƒ¹æ ¼
                        'market_cap_usd': float(tvl),
                        'volume_24h_usd': 0,
                        'chain': chain_name
                    })
            
            return price_data
            
        except Exception as e:
            logger.warning(f"ç²å–{chain_name}æ­·å²TVLæ•¸æ“šå¤±æ•—: {e}")
            # è¿”å›æœ€å°åŒ–çš„æ•¸æ“š
            current_date = datetime.now()
            return [{
                'date': (current_date - timedelta(days=i)).strftime('%Y-%m-%d'),
                'price_usd': 1.0,
                'market_cap_usd': 1e9,
                'volume_24h_usd': 0,
                'chain': chain_name
            } for i in range(30)]
    
    def _get_price_data(self, token_id, chain_name):
        """ç²å–ä»£å¹£åƒ¹æ ¼æ­·å²æ•¸æ“š"""
        try:
            # DefiLlamaåƒ¹æ ¼API (ç²å–365å¤©æ•¸æ“š)
            end_timestamp = int(datetime.now().timestamp())
            start_timestamp = int((datetime.now() - timedelta(days=365)).timestamp())
            
            url = f"https://coins.llama.fi/prices/historical/{start_timestamp}/{token_id}"
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            price_data = response.json()
            prices = []
            
            # å¦‚æœAPIæ ¼å¼ä¸åŒï¼Œå‰‡ä½¿ç”¨æ›¿ä»£æ–¹æ³•
            if 'prices' in price_data:
                for price_point in price_data['prices']:
                    prices.append({
                        'date': datetime.fromtimestamp(price_point[0]).strftime('%Y-%m-%d'),
                        'price_usd': float(price_point[1]),
                        'market_cap_usd': 0,  # éœ€è¦é¡å¤–APIèª¿ç”¨
                        'volume_24h_usd': 0,  # éœ€è¦é¡å¤–APIèª¿ç”¨
                        'chain': chain_name
                    })
            else:
                # ä½¿ç”¨ç•¶å‰åƒ¹æ ¼APIä½œç‚ºå¾Œå‚™
                current_url = f"https://api.llama.fi/coins/prices/current/{token_id}"
                current_response = requests.get(current_url, timeout=30)
                if current_response.status_code == 200:
                    current_data = current_response.json()
                    current_price = current_data.get('coins', {}).get(token_id, {}).get('price', 0)
                    
                    # ç”Ÿæˆæœ€è¿‘365å¤©çš„æ¨¡æ“¬æ•¸æ“šï¼ˆåŸºæ–¼ç•¶å‰åƒ¹æ ¼ï¼‰
                    for i in range(365):
                        date = datetime.now() - timedelta(days=i)
                        prices.append({
                            'date': date.strftime('%Y-%m-%d'),
                            'price_usd': float(current_price),
                            'market_cap_usd': 0,
                            'volume_24h_usd': 0,
                            'chain': chain_name
                        })
            
            return prices
            
        except Exception as e:
            logger.warning(f"ç²å–{chain_name}åƒ¹æ ¼æ•¸æ“šå¤±æ•—: {e}")
            # è¿”å›æœ€å°åŒ–çš„åƒ¹æ ¼æ•¸æ“š
            return [{
                'date': datetime.now().strftime('%Y-%m-%d'),
                'price_usd': 1.0,
                'market_cap_usd': 0,
                'volume_24h_usd': 0,
                'chain': chain_name
            }]
    
    def _save_raw_data_backup(self, data):
        """ä¿å­˜åŸå§‹æ•¸æ“šä½œç‚ºå‚™ä»½"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_dir = f"{self.raw_data_dir}/api_backup_{timestamp}"
            os.makedirs(backup_dir, exist_ok=True)
            
            data['sui_protocols'].to_csv(f"{backup_dir}/sui_protocols_api.csv", index=False)
            data['aptos_protocols'].to_csv(f"{backup_dir}/aptos_protocols_api.csv", index=False)
            data['sui_price'].to_csv(f"{backup_dir}/sui_price_api.csv", index=False)
            data['aptos_price'].to_csv(f"{backup_dir}/aptos_price_api.csv", index=False)
            
            with open(f"{backup_dir}/comparison_api.json", 'w', encoding='utf-8') as f:
                json.dump(data['comparison'], f, ensure_ascii=False, indent=2)
                
            logger.info(f"æ•¸æ“šå‚™ä»½ä¿å­˜åˆ°: {backup_dir}")
            
        except Exception as e:
            logger.warning(f"å‚™ä»½ä¿å­˜å¤±æ•—: {e}")
    
    def _load_backup_data(self):
        """è¼‰å…¥æœ¬åœ°å‚™ä»½æ•¸æ“šä½œç‚ºå¾Œå‚™"""
        try:
            # å˜—è©¦è¼‰å…¥æœ€æ–°çš„CSVæ–‡ä»¶
            data = {}
            data['sui_protocols'] = pd.read_csv(f"{self.raw_data_dir}/sui_data/sui_protocols_final_20250822.csv")
            data['aptos_protocols'] = pd.read_csv(f"{self.raw_data_dir}/aptos_data/aptos_protocols_final_20250822.csv")
            data['sui_price'] = pd.read_csv(f"{self.raw_data_dir}/sui_data/sui_price_final_20250822.csv")
            data['aptos_price'] = pd.read_csv(f"{self.raw_data_dir}/aptos_data/aptos_price_final_20250822.csv")
            
            with open(f"{self.raw_data_dir}/comparison_data/sui_vs_aptos_final_20250822.json", 'r', encoding='utf-8') as f:
                data['comparison'] = json.load(f)
                
            logger.info("å·²è¼‰å…¥æœ¬åœ°å‚™ä»½æ•¸æ“š")
            return data
            
        except Exception as e:
            logger.error(f"æœ¬åœ°å‚™ä»½æ•¸æ“šè¼‰å…¥ä¹Ÿå¤±æ•—: {e}")
            return None
    
    def clean_protocol_data(self, protocols_df, chain_name):
        """æ¸…ç†å”è­°æ•¸æ“š - å¼·åŒ–CEXéæ¿¾"""
        logger.info(f"é–‹å§‹æ¸…ç† {chain_name} å”è­°æ•¸æ“š...")
        
        # è¤‡è£½æ•¸æ“šé¿å…ä¿®æ”¹åŸå§‹æ•¸æ“š
        df = protocols_df.copy()
        initial_count = len(df)
        
        # 1. å¼·åŒ–CEXå’ŒéDeFiå”è­°éæ¿¾
        # æ“´å±•æ’é™¤é—œéµè©åˆ—è¡¨ï¼ŒåŒ…å«æ‰€æœ‰å¯èƒ½çš„CEX
        exclude_keywords = [
            # ä¸»è¦CEX
            'Binance', 'binance', 'BINANCE',
            'OKX', 'okx', 'OKEx', 
            'Bybit', 'bybit', 'BYBIT',
            'Gate', 'gate.io', 'GATE',
            'HTX', 'htx', 'Huobi', 'huobi',
            'KuCoin', 'kucoin', 'KUCOIN',
            'Bitfinex', 'bitfinex', 'BITFINEX',
            'Bitstamp', 'bitstamp', 'BITSTAMP',
            'MEXC', 'mexc',
            'Kraken', 'kraken', 'KRAKEN',
            'Coinbase', 'coinbase', 'COINBASE',
            
            # å…¶ä»–CEXå’Œäº¤æ˜“æ‰€
            'HashKey Exchange', 'hashkey',
            'Indodax', 'indodax',
            'Phemex', 'phemex',
            'FTX', 'ftx',
            'Crypto.com', 'crypto.com',
            'Bitget', 'bitget',
            'XT.com', 'xt.com',
            
            # é€šç”¨é—œéµè©
            'CEX', 'cex',
            'Exchange', 'exchange', 'EXCHANGE',
            'Trading', 'trading',
            
            # å¯ç–‘çš„éDeFiå”è­°
            'Custody', 'custody',
            'Wallet', 'wallet' # æ³¨æ„ï¼šé€™å¯èƒ½éæ–¼å¯¬æ³›ï¼Œéœ€è¦è¬¹æ…
        ]
        
        # é€å€‹é—œéµè©éæ¿¾ï¼Œè¨˜éŒ„æ¯å€‹æ­¥é©Ÿ
        for keyword in exclude_keywords:
            before_count = len(df)
            df = df[~df['name'].str.contains(keyword, case=False, na=False)]
            removed = before_count - len(df)
            if removed > 0:
                logger.info(f"  ç§»é™¤åŒ…å« '{keyword}' çš„å”è­°: {removed} å€‹")
        
        # 2. é¡å¤–çš„æ•¸å€¼é©—è­‰ - ç§»é™¤ç•°å¸¸é«˜çš„TVLï¼ˆå¯èƒ½æ˜¯éŒ¯èª¤æ•¸æ“šï¼‰
        # è¨­å®šåˆç†çš„TVLä¸Šé™ï¼Œä»»ä½•è¶…é100Bçš„å”è­°éƒ½å¯ç–‘
        tvl_upper_limit = 100e9  # 100å„„ç¾å…ƒ
        before_outlier = len(df)
        df = df[df['tvl_usd'] < tvl_upper_limit]
        outlier_removed = before_outlier - len(df)
        if outlier_removed > 0:
            logger.info(f"  ç§»é™¤TVLç•°å¸¸é«˜çš„å”è­° (>{tvl_upper_limit/1e9:.0f}B): {outlier_removed} å€‹")
        
        # 3. è™•ç†TVLç•°å¸¸å€¼
        # ç§»é™¤TVLç‚º0æˆ–è² æ•¸çš„å”è­°
        df = df[df['tvl_usd'] > 0]
        
        # æ¨™è¨˜TVLç•°å¸¸é«˜çš„å”è­° (å¯èƒ½æ˜¯æ•¸æ“šéŒ¯èª¤)
        if len(df) > 0:
            tvl_q99 = df['tvl_usd'].quantile(0.99)
            df['is_outlier'] = df['tvl_usd'] > tvl_q99
            logger.info(f"  æ¨™è¨˜TVLç•°å¸¸å€¼: {df['is_outlier'].sum()} å€‹ (>99åˆ†ä½æ•¸: ${tvl_q99:,.0f})")
        
        # 4. åˆ†é¡æ¨™æº–åŒ–
        category_mapping = {
            'Dexes': 'DEX',
            'Derivatives': 'Derivatives', 
            'Lending': 'Lending',
            'Yield': 'Yield Farming',
            'Liquid Staking': 'Liquid Staking',
            'Bridge': 'Bridge',
            'Launchpad': 'Launchpad',
            'Gaming': 'Gaming',
            'NFT': 'NFT',
            'Insurance': 'Insurance',
            'RWA': 'RWA',  # æ·»åŠ RWAåˆ†é¡
            'Unknown': 'Others'
        }
        
        df['category_clean'] = df['category'].map(category_mapping).fillna('Others')
        
        # 5. æ·»åŠ è¨ˆç®—æ¬„ä½
        df['tvl_millions'] = df['tvl_usd'] / 1_000_000
        df['tvl_billions'] = df['tvl_usd'] / 1_000_000_000
        
        # 6. è¨ˆç®—å¢é•·è©•åˆ† (åŸºæ–¼è®ŠåŒ–ç‡)
        df['growth_score'] = (
            df['change_7d'].fillna(0) * 0.5 + 
            df['change_1m'].fillna(0) * 0.3 + 
            df['change_1d'].fillna(0) * 0.2
        )
        
        # 7. æ·»åŠ æ’å
        df['tvl_rank'] = df['tvl_usd'].rank(method='dense', ascending=False)
        df['growth_rank'] = df['growth_score'].rank(method='dense', ascending=False)
        
        # 8. æœ€çµ‚é©—è­‰ - é¡¯ç¤ºæ¸…ç†çµæœ
        final_count = len(df)
        removed_total = initial_count - final_count
        
        logger.info(f"  {chain_name} æ¸…ç†æ‘˜è¦:")
        logger.info(f"    åŸå§‹å”è­°æ•¸: {initial_count}")
        logger.info(f"    ç§»é™¤å”è­°æ•¸: {removed_total}")
        logger.info(f"    æœ€çµ‚å”è­°æ•¸: {final_count}")
        
        if final_count > 0:
            logger.info(f"    TVLç¸½å’Œ: ${df['tvl_usd'].sum()/1e9:.2f}B")
            logger.info(f"    æœ€å¤§å”è­°TVL: ${df['tvl_usd'].max()/1e6:.1f}M")
            
            # é¡¯ç¤ºå‰5å¤§å”è­°ä»¥ä¾›é©—è­‰
            top_5 = df.nlargest(5, 'tvl_usd')
            logger.info(f"    å‰5å¤§å”è­°:")
            for idx, row in top_5.iterrows():
                logger.info(f"      {row['name']}: ${row['tvl_usd']/1e6:.1f}M ({row['category']})")
        
        return df
    
    def clean_price_data(self, price_df, chain_name):
        """æ¸…ç†åƒ¹æ ¼æ•¸æ“š"""
        logger.info(f"é–‹å§‹æ¸…ç† {chain_name} åƒ¹æ ¼æ•¸æ“š...")
        
        df = price_df.copy()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰åƒ¹æ ¼æ•¸æ“š
        if len(df) == 0:
            logger.warning(f"{chain_name} æ²’æœ‰åƒ¹æ ¼æ•¸æ“šï¼Œå‰µå»ºæœ€å°åŒ–æ•¸æ“šé›†")
            # å‰µå»ºæœ€å°åŒ–çš„åƒ¹æ ¼æ•¸æ“š
            current_date = datetime.now()
            minimal_data = []
            for i in range(30):  # å‰µå»º30å¤©çš„åŸºç¤æ•¸æ“š
                date = current_date - timedelta(days=i)
                minimal_data.append({
                    'date': date.strftime('%Y-%m-%d'),
                    'price_usd': 1.0,  # ä½¿ç”¨é»˜èªåƒ¹æ ¼
                    'market_cap_usd': 0,
                    'volume_24h_usd': 0,
                    'chain': chain_name
                })
            df = pd.DataFrame(minimal_data)
        
        # 1. è½‰æ›æ—¥æœŸæ ¼å¼
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date').reset_index(drop=True)
        else:
            logger.error(f"{chain_name} åƒ¹æ ¼æ•¸æ“šç¼ºå°‘ 'date' æ¬„ä½")
            return pd.DataFrame()  # è¿”å›ç©ºDataFrame
        
        # 2. è™•ç†ç¼ºå¤±å€¼
        numeric_cols = ['price_usd', 'market_cap_usd', 'volume_24h_usd']
        for col in numeric_cols:
            if col in df.columns:
                # ä½¿ç”¨å‰å€¼å¡«å……å°é‡ç¼ºå¤±å€¼
                df[col] = df[col].fillna(method='ffill')
                # å¦‚æœé‚„æœ‰ç¼ºå¤±å€¼ï¼Œç”¨0å¡«å……
                df[col] = df[col].fillna(0)
        
        # 3. è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
        if len(df) > 1:
            df['price_change_1d'] = df['price_usd'].pct_change()
            df['price_change_7d'] = df['price_usd'].pct_change(periods=min(7, len(df)-1))
            df['price_change_30d'] = df['price_usd'].pct_change(periods=min(30, len(df)-1))
            
            # 4. ç§»å‹•å¹³å‡
            df['ma_7d'] = df['price_usd'].rolling(window=min(7, len(df))).mean()
            df['ma_30d'] = df['price_usd'].rolling(window=min(30, len(df))).mean()
            
            # 5. æ³¢å‹•ç‡ (30å¤©æ»¾å‹•æ¨™æº–å·®)
            df['volatility_30d'] = df['price_change_1d'].rolling(window=min(30, len(df))).std()
            
            # 6. ç´¯ç©å›å ±
            df['cumulative_return'] = (df['price_usd'] / df['price_usd'].iloc[0] - 1) * 100
        else:
            # å¦‚æœåªæœ‰ä¸€è¡Œæ•¸æ“šï¼Œè¨­ç½®é»˜èªå€¼
            df['price_change_1d'] = 0
            df['price_change_7d'] = 0
            df['price_change_30d'] = 0
            df['ma_7d'] = df['price_usd']
            df['ma_30d'] = df['price_usd']
            df['volatility_30d'] = 0
            df['cumulative_return'] = 0
        
        # 7. æ·»åŠ æ™‚é–“ç‰¹å¾µ
        df['year'] = df['date'].dt.year
        df['month'] = df['date'].dt.month
        df['quarter'] = df['date'].dt.quarter
        df['weekday'] = df['date'].dt.dayofweek
        
        logger.info(f"  {chain_name} åƒ¹æ ¼æ•¸æ“šæ¸…ç†å®Œæˆ: {len(df)} å¤©")
        
        return df
    
    def create_comparative_analysis(self, sui_protocols_clean, aptos_protocols_clean, sui_price_clean, aptos_price_clean):
        """å‰µå»ºæ¯”è¼ƒåˆ†ææ•¸æ“šé›†"""
        logger.info("é–‹å§‹å‰µå»ºæ¯”è¼ƒåˆ†ææ•¸æ“šé›†...")
        
        analysis = {}
        
        # 1. å”è­°æ¯”è¼ƒ
        protocol_comparison = {
            'category_comparison': self._compare_categories(sui_protocols_clean, aptos_protocols_clean),
            'size_distribution': self._analyze_size_distribution(sui_protocols_clean, aptos_protocols_clean),
            'growth_comparison': self._compare_growth_metrics(sui_protocols_clean, aptos_protocols_clean),
            'concentration_analysis': self._analyze_concentration(sui_protocols_clean, aptos_protocols_clean)
        }
        
        # 2. åƒ¹æ ¼æ¯”è¼ƒ
        price_comparison = {
            'performance_metrics': self._compare_price_performance(sui_price_clean, aptos_price_clean),
            'risk_metrics': self._compare_risk_metrics(sui_price_clean, aptos_price_clean),
            'correlation_analysis': self._analyze_price_correlation(sui_price_clean, aptos_price_clean)
        }
        
        # 3. ç¶œåˆè©•åˆ† - ä¿®æ­£è©•åˆ†é‚è¼¯ä»¥åæ˜ å¯¦éš›é ˜å…ˆè€…
        ecosystem_scores = self._calculate_ecosystem_scores(
            sui_protocols_clean, aptos_protocols_clean, 
            sui_price_clean, aptos_price_clean
        )
        
        analysis = {
            'protocol_comparison': protocol_comparison,
            'price_comparison': price_comparison,
            'ecosystem_scores': ecosystem_scores,
            'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        return analysis
    
    def _compare_categories(self, sui_df, aptos_df):
        """æ¯”è¼ƒå”è­°åˆ†é¡åˆ†ä½ˆ"""
        sui_categories = sui_df.groupby('category_clean').agg({
            'tvl_usd': ['sum', 'count', 'mean']
        }).round(0)
        
        aptos_categories = aptos_df.groupby('category_clean').agg({
            'tvl_usd': ['sum', 'count', 'mean']
        }).round(0)
        
        # è½‰æ›ç‚ºç°¡å–®å­—å…¸æ ¼å¼
        sui_cat_dict = {}
        aptos_cat_dict = {}
        
        for category in sui_categories.index:
            sui_cat_dict[category] = {
                'total_tvl': float(sui_categories.loc[category, ('tvl_usd', 'sum')]),
                'protocol_count': int(sui_categories.loc[category, ('tvl_usd', 'count')]),
                'avg_tvl': float(sui_categories.loc[category, ('tvl_usd', 'mean')])
            }
        
        for category in aptos_categories.index:
            aptos_cat_dict[category] = {
                'total_tvl': float(aptos_categories.loc[category, ('tvl_usd', 'sum')]),
                'protocol_count': int(aptos_categories.loc[category, ('tvl_usd', 'count')]),
                'avg_tvl': float(aptos_categories.loc[category, ('tvl_usd', 'mean')])
            }
        
        return {
            'sui_categories': sui_cat_dict,
            'aptos_categories': aptos_cat_dict
        }
    
    def _analyze_size_distribution(self, sui_df, aptos_df):
        """åˆ†æå”è­°è¦æ¨¡åˆ†ä½ˆ"""
        size_brackets = [0, 1e6, 10e6, 100e6, 1e9, float('inf')]
        size_labels = ['<$1M', '$1M-$10M', '$10M-$100M', '$100M-$1B', '>$1B']
        
        sui_df['size_bracket'] = pd.cut(sui_df['tvl_usd'], bins=size_brackets, labels=size_labels)
        aptos_df['size_bracket'] = pd.cut(aptos_df['tvl_usd'], bins=size_brackets, labels=size_labels)
        
        sui_dist = sui_df['size_bracket'].value_counts().to_dict()
        aptos_dist = aptos_df['size_bracket'].value_counts().to_dict()
        
        return {
            'sui_distribution': {str(k): int(v) for k, v in sui_dist.items()},
            'aptos_distribution': {str(k): int(v) for k, v in aptos_dist.items()}
        }
    
    def _compare_growth_metrics(self, sui_df, aptos_df):
        """æ¯”è¼ƒå¢é•·æŒ‡æ¨™"""
        return {
            'sui_growth': {
                'avg_7d_change': float(sui_df['change_7d'].mean()),
                'avg_30d_change': float(sui_df['change_1m'].mean()),
                'positive_growth_7d': int((sui_df['change_7d'] > 0).sum()),
                'negative_growth_7d': int((sui_df['change_7d'] < 0).sum())
            },
            'aptos_growth': {
                'avg_7d_change': float(aptos_df['change_7d'].mean()),
                'avg_30d_change': float(aptos_df['change_1m'].mean()),
                'positive_growth_7d': int((aptos_df['change_7d'] > 0).sum()),
                'negative_growth_7d': int((aptos_df['change_7d'] < 0).sum())
            }
        }
    
    def _analyze_concentration(self, sui_df, aptos_df):
        """åˆ†æç”Ÿæ…‹é›†ä¸­åº¦"""
        sui_total = sui_df['tvl_usd'].sum()
        aptos_total = aptos_df['tvl_usd'].sum()
        
        return {
            'sui_concentration': {
                'top_1_share': float(sui_df.iloc[0]['tvl_usd'] / sui_total * 100),
                'top_5_share': float(sui_df.head(5)['tvl_usd'].sum() / sui_total * 100),
                'top_10_share': float(sui_df.head(10)['tvl_usd'].sum() / sui_total * 100),
                'herfindahl_index': float((sui_df['tvl_usd'] / sui_total).pow(2).sum())
            },
            'aptos_concentration': {
                'top_1_share': float(aptos_df.iloc[0]['tvl_usd'] / aptos_total * 100),
                'top_5_share': float(aptos_df.head(5)['tvl_usd'].sum() / aptos_total * 100),
                'top_10_share': float(aptos_df.head(10)['tvl_usd'].sum() / aptos_total * 100),
                'herfindahl_index': float((aptos_df['tvl_usd'] / aptos_total).pow(2).sum())
            }
        }
    
    def _compare_price_performance(self, sui_price, aptos_price):
        """æ¯”è¼ƒåƒ¹æ ¼è¡¨ç¾"""
        periods = [7, 30, 90, 180]
        performance = {}
        
        for period in periods:
            if len(sui_price) > period and len(aptos_price) > period:
                sui_return = ((sui_price.iloc[-1]['price_usd'] - sui_price.iloc[-period]['price_usd']) 
                             / sui_price.iloc[-period]['price_usd'] * 100)
                aptos_return = ((aptos_price.iloc[-1]['price_usd'] - aptos_price.iloc[-period]['price_usd']) 
                               / aptos_price.iloc[-period]['price_usd'] * 100)
                
                performance[f'{period}_days'] = {
                    'sui_return': float(sui_return),
                    'aptos_return': float(aptos_return),
                    'outperformance': float(sui_return - aptos_return)
                }
        
        return performance
    
    def _compare_risk_metrics(self, sui_price, aptos_price):
        """æ¯”è¼ƒé¢¨éšªæŒ‡æ¨™"""
        return {
            'volatility_30d': {
                'sui': float(sui_price['volatility_30d'].iloc[-1]),
                'aptos': float(aptos_price['volatility_30d'].iloc[-1])
            },
            'max_drawdown': {
                'sui': float((sui_price['price_usd'].cummax() - sui_price['price_usd']).max() / sui_price['price_usd'].cummax().max() * 100),
                'aptos': float((aptos_price['price_usd'].cummax() - aptos_price['price_usd']).max() / aptos_price['price_usd'].cummax().max() * 100)
            }
        }
    
    def _analyze_price_correlation(self, sui_price, aptos_price):
        """åˆ†æåƒ¹æ ¼ç›¸é—œæ€§"""
        # åˆä½µæ•¸æ“šä»¥è¨ˆç®—ç›¸é—œæ€§
        merged = pd.merge(sui_price[['date', 'price_change_1d']], 
                         aptos_price[['date', 'price_change_1d']], 
                         on='date', suffixes=('_sui', '_aptos'))
        
        correlation = merged['price_change_1d_sui'].corr(merged['price_change_1d_aptos'])
        
        return {
            'daily_correlation': float(correlation),
            'correlation_strength': 'High' if abs(correlation) > 0.7 else 'Medium' if abs(correlation) > 0.4 else 'Low'
        }
    
    def _calculate_ecosystem_scores(self, sui_protocols, aptos_protocols, sui_price, aptos_price):
        """è¨ˆç®—ç”Ÿæ…‹ç³»çµ±ç¶œåˆè©•åˆ† - ä¿®æ­£ç‚ºä»¥å¯¦éš›é ˜å…ˆè€…ç‚ºåŸºæº–"""
        
        # TVLè©•åˆ† - ä»¥å¯¦éš›æ›´é«˜è€…ç‚º100åˆ†åŸºæº–
        sui_tvl = sui_protocols['tvl_usd'].sum()
        aptos_tvl = aptos_protocols['tvl_usd'].sum()
        
        if sui_tvl > aptos_tvl:
            # Suié ˜å…ˆçš„æƒ…æ³
            tvl_score_sui = 100.0
            tvl_score_aptos = (aptos_tvl / sui_tvl) * 100
        else:
            # Aptosé ˜å…ˆçš„æƒ…æ³
            tvl_score_aptos = 100.0
            tvl_score_sui = (sui_tvl / aptos_tvl) * 100
        
        # å¤šæ¨£æ€§è©•åˆ† (å”è­°æ•¸é‡)
        diversity_score_sui = len(sui_protocols)
        diversity_score_aptos = len(aptos_protocols)
        
        # å¢é•·è©•åˆ† (åŸºæ–¼90å¤©åƒ¹æ ¼è¡¨ç¾)
        sui_90d_return = ((sui_price.iloc[-1]['price_usd'] - sui_price.iloc[-91]['price_usd']) 
                         / sui_price.iloc[-91]['price_usd'] * 100) if len(sui_price) > 90 else 0
        aptos_90d_return = ((aptos_price.iloc[-1]['price_usd'] - aptos_price.iloc[-91]['price_usd']) 
                           / aptos_price.iloc[-91]['price_usd'] * 100) if len(aptos_price) > 90 else 0
        
        # æ¨™æº–åŒ–è©•åˆ† (0-100)
        growth_score_sui = max(0, min(100, 50 + sui_90d_return))
        growth_score_aptos = max(0, min(100, 50 + aptos_90d_return))
        
        # ç¶œåˆè©•åˆ†
        sui_overall = (tvl_score_sui * 0.4 + diversity_score_sui * 0.3 + growth_score_sui * 0.3)
        aptos_overall = (tvl_score_aptos * 0.4 + diversity_score_aptos * 0.3 + growth_score_aptos * 0.3)
        
        return {
            'sui_scores': {
                'tvl_score': float(tvl_score_sui),
                'diversity_score': float(diversity_score_sui),
                'growth_score': float(growth_score_sui),
                'overall_score': float(sui_overall)
            },
            'aptos_scores': {
                'tvl_score': float(tvl_score_aptos),
                'diversity_score': float(diversity_score_aptos),
                'growth_score': float(growth_score_aptos),
                'overall_score': float(aptos_overall)
            }
        }
    
    def save_processed_data(self, processed_data):
        """ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š"""
        logger.info("é–‹å§‹ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š...")
        
        timestamp = datetime.now().strftime('%Y%m%d')
        
        # ä¿å­˜æ¸…ç†å¾Œçš„æ•¸æ“š
        processed_data['sui_protocols_clean'].to_csv(
            f"{self.output_dir}/daily/sui_protocols_clean_{timestamp}.csv", index=False)
        
        processed_data['aptos_protocols_clean'].to_csv(
            f"{self.output_dir}/daily/aptos_protocols_clean_{timestamp}.csv", index=False)
        
        processed_data['sui_price_clean'].to_csv(
            f"{self.output_dir}/daily/sui_price_clean_{timestamp}.csv", index=False)
        
        processed_data['aptos_price_clean'].to_csv(
            f"{self.output_dir}/daily/aptos_price_clean_{timestamp}.csv", index=False)
        
        # ä¿å­˜æ¯”è¼ƒåˆ†æ
        with open(f"{self.output_dir}/analysis_ready/comparative_analysis_{timestamp}.json", 'w', encoding='utf-8') as f:
            json.dump(processed_data['comparative_analysis'], f, ensure_ascii=False, indent=2)
        
        logger.info("è™•ç†å¾Œæ•¸æ“šä¿å­˜å®Œæˆ")
    
    def process_all_data(self):
        """åŸ·è¡Œå®Œæ•´çš„æ•¸æ“šè™•ç†æµç¨‹"""
        logger.info("=== é–‹å§‹æ•¸æ“šè™•ç†æµç¨‹ ===")
        
        # 1. è¼‰å…¥åŸå§‹æ•¸æ“š
        raw_data = self.load_raw_data()
        if raw_data is None:
            logger.error("åŸå§‹æ•¸æ“šè¼‰å…¥å¤±æ•—ï¼Œçµ‚æ­¢è™•ç†")
            return None
        
        # 2. æ¸…ç†å”è­°æ•¸æ“š
        sui_protocols_clean = self.clean_protocol_data(raw_data['sui_protocols'], 'Sui')
        aptos_protocols_clean = self.clean_protocol_data(raw_data['aptos_protocols'], 'Aptos')
        
        # 3. æ¸…ç†åƒ¹æ ¼æ•¸æ“š
        sui_price_clean = self.clean_price_data(raw_data['sui_price'], 'Sui')
        aptos_price_clean = self.clean_price_data(raw_data['aptos_price'], 'Aptos')
        
        # 4. å‰µå»ºæ¯”è¼ƒåˆ†æ
        comparative_analysis = self.create_comparative_analysis(
            sui_protocols_clean, aptos_protocols_clean,
            sui_price_clean, aptos_price_clean
        )
        
        # 5. æ•´åˆè™•ç†å¾Œæ•¸æ“š
        processed_data = {
            'sui_protocols_clean': sui_protocols_clean,
            'aptos_protocols_clean': aptos_protocols_clean,
            'sui_price_clean': sui_price_clean,
            'aptos_price_clean': aptos_price_clean,
            'comparative_analysis': comparative_analysis
        }
        
        # 6. ä¿å­˜è™•ç†å¾Œæ•¸æ“š
        self.save_processed_data(processed_data)
        
        logger.info("=== æ•¸æ“šè™•ç†æµç¨‹å®Œæˆ ===")
        return processed_data

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸ”§ Sui vs Aptos æ•¸æ“šè™•ç†å·¥å…· (ä¿®æ­£ç‰ˆ)")
    print(f"é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 50)
    
    # å‰µå»ºæ•¸æ“šè™•ç†å™¨
    processor = SuiAptosDataProcessor()
    
    try:
        # åŸ·è¡Œæ•¸æ“šè™•ç†
        processed_data = processor.process_all_data()
        
        if processed_data:
            print("\nğŸ“Š æ•¸æ“šè™•ç†æ‘˜è¦:")
            print("=" * 40)
            
            # é¡¯ç¤ºæ¸…ç†å¾Œçš„æ•¸æ“šçµ±è¨ˆ
            sui_clean = processed_data['sui_protocols_clean']
            aptos_clean = processed_data['aptos_protocols_clean']
            
            sui_tvl = sui_clean['tvl_usd'].sum() / 1e9
            aptos_tvl = aptos_clean['tvl_usd'].sum() / 1e9
            
            print(f"âœ… Suiå”è­° (æ¸…ç†å¾Œ): {len(sui_clean)} å€‹, TVL: ${sui_tvl:.2f}B")
            print(f"âœ… Aptoså”è­° (æ¸…ç†å¾Œ): {len(aptos_clean)} å€‹, TVL: ${aptos_tvl:.2f}B")
            
            leader = "Sui" if sui_tvl > aptos_tvl else "Aptos"
            ratio = max(sui_tvl, aptos_tvl) / min(sui_tvl, aptos_tvl)
            print(f"ğŸ“ˆ TVLé ˜å…ˆè€…: {leader} (é ˜å…ˆ {ratio:.1f}å€)")
            
            print(f"âœ… SUIåƒ¹æ ¼æ•¸æ“š: {len(processed_data['sui_price_clean'])} å¤©")
            print(f"âœ… APTåƒ¹æ ¼æ•¸æ“š: {len(processed_data['aptos_price_clean'])} å¤©")
            
            # é¡¯ç¤ºä¸€äº›é—œéµæ´å¯Ÿ
            comp_analysis = processed_data['comparative_analysis']
            if 'ecosystem_scores' in comp_analysis:
                scores = comp_analysis['ecosystem_scores']
                print(f"\nğŸ† ç”Ÿæ…‹ç³»çµ±è©•åˆ†:")
                print(f"  Sui ç¶œåˆè©•åˆ†: {scores['sui_scores']['overall_score']:.1f}")
                print(f"  Aptos ç¶œåˆè©•åˆ†: {scores['aptos_scores']['overall_score']:.1f}")
            
            print(f"\nğŸ“ è™•ç†å¾Œæ•¸æ“šä¿å­˜ä½ç½®: {processor.output_dir}/")
            print(f"ğŸ• å®Œæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            return 0
        else:
            print("âŒ æ•¸æ“šè™•ç†å¤±æ•—")
            return 1
            
    except Exception as e:
        logger.error(f"æ•¸æ“šè™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)